{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78125afa",
   "metadata": {},
   "source": [
    "# Code Samples\n",
    "\n",
    "## Preparations\n",
    "\n",
    "For using small gradio examples, we need to setup a `conda` environment and install the required libraries.\n",
    "\n",
    "```\n",
    "# 1. Create a new conda environment with Python 3.10\n",
    "conda create -n gradio-env python=3.10 -y\n",
    "\n",
    "# 2. Activate the environment\n",
    "conda activate gradio-env\n",
    "\n",
    "# 3. Install Torch (CPU‐only) via pip\n",
    "pip install torch torchvision torchaudio\n",
    "\n",
    "# 4. Install the remaining libraries\n",
    "pip install gradio transformers pillow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4516214",
   "metadata": {},
   "source": [
    "### Captioning\n",
    "\n",
    "Image captioning is the task of automatically generating a natural-language description for a given image. It sits at the intersection of computer vision (to “see” and interpret the picture) and natural language processing (to “speak” or “write” about what is seen).\n",
    "\n",
    "Create a new file called `captioning.py` and copy the code below.\n",
    "\n",
    "Then in a new terminal run the following two commands:\n",
    "\n",
    "`conda activate gradio-env`\n",
    "\n",
    "`python captioning.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b54c696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# 1. Load BLIP processor and model\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model.eval()\n",
    "\n",
    "# 2. Define a captioning function\n",
    "def generate_caption(image: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Takes a PIL Image from Gradio’s camera input and returns a text caption.\n",
    "    \"\"\"\n",
    "    # Preprocess the image\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate caption (you can adjust num_beams, max_length, etc. as needed)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            num_beams=5,\n",
    "            max_length=20\n",
    "        )\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "# 3. Set up Gradio interface with a webcam (camera) input\n",
    "camera_input = gr.Image(type=\"pil\")\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=generate_caption,\n",
    "    inputs=camera_input,\n",
    "    outputs=\"text\",\n",
    "    title=\"BLIP Image Captioning (Webcam)\",\n",
    "    description=\"Point your webcam at something, click 'Submit', and BLIP will describe what it sees.\"\n",
    ")\n",
    "\n",
    "# 4. Launch the app\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf7828f",
   "metadata": {},
   "source": [
    "\n",
    "### Visual Question Answering\n",
    "\n",
    "Visual Question Answering (VQA) is a multimodal task that combines computer vision and natural language processing to answer questions about images. Given an image and a free‐form question (“What color is the cat?”; “Who is holding an umbrella?”; “How many people are in the kitchen?”), a VQA system must return a concise, correct answer in natural language (often a single word or short phrase).\n",
    "\n",
    "Create a new file called `vqa.py` and copy the code below.\n",
    "\n",
    "Then in a new terminal run the following two commands:\n",
    "\n",
    "`conda activate gradio-env`\n",
    "\n",
    "`python vqa.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c09f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "\n",
    "# 1. Load BLIP VQA processor and model\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "model.eval()\n",
    "\n",
    "# 2. Define a VQA‐style function\n",
    "def answer_question(image: Image.Image, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Takes a PIL Image and a text question, \n",
    "    returns BLIP’s predicted answer.\n",
    "    \"\"\"\n",
    "    # Preprocess both image and text\n",
    "    inputs = processor(images=image, text=question, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate answer (you can tune num_beams, max_length, etc.)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            num_beams=5,\n",
    "            max_length=10\n",
    "        )\n",
    "    answer = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "# 3. Set up Gradio interface with a camera input + text box\n",
    "camera_input = gr.Image(type=\"pil\", label=\"Webcam Image\")\n",
    "text_input = gr.Textbox(lines=2, placeholder=\"Ask a question about the image…\", label=\"Question\")\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=answer_question,\n",
    "    inputs=[camera_input, text_input],\n",
    "    outputs=\"text\",\n",
    "    title=\"BLIP VQA (Webcam)\",\n",
    "    description=\"Point your webcam at something, type a question, click 'Submit', and BLIP will answer.\"\n",
    ")\n",
    "\n",
    "# 4. Launch the app\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ebcfdb",
   "metadata": {},
   "source": [
    "\n",
    "### Large Multimodal Models\n",
    "\n",
    "Larger multimodal models combine the generative qualities of large language models and vision models.\n",
    "\n",
    "We use [LLaVa](https://llava-vl.github.io/) together with `ollama`.\n",
    "\n",
    "First install `ollama`. See section [Ollama](https://attributeerror39.github.io/experimental-ai-lab/llm/ollama_start.html).\n",
    "\n",
    "Create a new file called `llava.py` and copy the code below.\n",
    "\n",
    "Then in a new terminal run the following two commands:\n",
    "\n",
    "`ollama pull llava:latest`\n",
    "\n",
    "`conda activate gradio-env`\n",
    "\n",
    "`pip install ollama`\n",
    "\n",
    "`python llava.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cb686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import tempfile\n",
    "import ollama\n",
    "\n",
    "def llava_via_ollama_python(image: Image.Image, question: str) -> str:\n",
    "    # 1. Save the incoming PIL Image to a temporary PNG on disk\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as tmp:\n",
    "        image.save(tmp.name)\n",
    "        image_path = tmp.name\n",
    "\n",
    "    # 2. Call ollama.chat with model=\"llava\", \n",
    "    #    putting the temp‐file path in the \"images\" list.\n",
    "    #\n",
    "    #    According to the Ollama Python docs, you can pass a path-like string\n",
    "    #    in the \"images\" field, and the model will load that file as input. :contentReference[oaicite:0]{index=0}\n",
    "    response = ollama.chat(\n",
    "        model=\"llava\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question,\n",
    "            \"images\": [image_path]\n",
    "        }]\n",
    "    )\n",
    "\n",
    "    # 3. Extract LLaVA’s answer from the response object\n",
    "    #    The .message.content field contains the text output. :contentReference[oaicite:1]{index=1}\n",
    "    return response.message.content.strip()\n",
    "\n",
    "# 4. Build a Gradio interface: one Image input + one Textbox, output plain text.\n",
    "iface = gr.Interface(\n",
    "    fn=llava_via_ollama_python,\n",
    "    inputs=[\n",
    "        gr.Image(type=\"pil\", label=\"Image\"),\n",
    "        gr.Textbox(lines=2, placeholder=\"Ask a question about the image…\", label=\"Question\")\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"LLaVA via Ollama (Python SDK)\",\n",
    "    description=(\n",
    "        \"Upload or capture an image and type a question. \"\n",
    "        \"Under the hood, this saves the image to a temp file and calls:\\n\\n\"\n",
    "        \"`ollama.chat(model='llava', messages=[{ 'role':'user', 'content':<question>, 'images':[<temp_path>] }])`\"\n",
    "    )\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
